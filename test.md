# Implementation

## Course Level: What kinds of courses is it appropriate for?

Upper-level,Intermediate,Intro College,High School

## Content: What does it test?

Focus:
:   Beliefs / Attitudes

Subject:

:   

## Timing: How long should I give students to take it?

20-30 min

## Example Questions

Sample questions from the MPEX:A significant problem in this course is
being able to memorize all the information I need to know.    *Strongly
Disagree   1  2   3   4   5   Strongly Agree*Knowledge in physics
consists of many pieces of information each of which applies primarily
to a specific situation.     *Strongly Disagree   1  2   3   4   5  
Strongly Agree*

## Versions and Variations: Which version of the test should I use?

The latest version of the MPEX, released in 1997, is version 4.0.

## Administering: How do I give the test?

-   Give it as both a pre- and post-test. This measures<span> how your
    class shifts student thinking.</span>
    -   Give the pre-test<span> at the beginning of the term</span>.
    -   Give the post-test at the end of the term.
-   Use the whole test, with the original wording and question order.
    This makes comparisons with other classes meaningful.
-   Make the test required, and give credit for completing the test (but
    not correctness). This ensures maximum participation from
    your students.
-   Tell your students that the test is designed to evaluate the course
    (not them), and that knowing how they think will help you
    teach better. Tell them that correctness will not affect their
    grades (only participation). This helps alleviate student anxiety.
-   For more details, read the **PhysPort Guides **on implementation:
    -   **PhysPort MPEX implementation guide**
        ([www.physport.org/implementation/MPEX](../implementation/MPEX))
    -   **PhysPort Expert Recommendation on Best Practices for
        Administering Belief Surveys**
        ([www.physport.org/expert/AdministeringBeliefSurveys/](../expert/AdministeringBeliefSurveys/))

## Scoring: How do I calculate my students' scores?

-   <span>The “percent favorable score” is the percentage of questions
    where a student agrees with the expert response. (Dis)agree and
    strongly (dis)agree are counted as equivalent responses. See
    the </span>**PhysPort MPEX implementation Guide**<span> for
    instructions on scoring the
    MPEX <span>(</span>[www.physport.org/implementation/MPEX](../implementation/MPEX)<span>)</span></span>
-   See the **PhysPort Expert Recommendation on Best Practices for
    Administering Belief Surveys** for instructions on calculating shift
    and effect size
    ([www.physport.org/expert/AdministeringBeliefSurveys/](../expert/AdministeringBeliefSurveys/))
-   Use the **PhysPort Assessment Data Explorer** for analysis and
    visualization of your students' responses
    ([www.physport.org/explore/MPEX](../explore/MPEX))

## Typical Results: What scores are usually achieved?

 In typical physics classes, students’ beliefs usually deteriorate or at
best stay the same. There are a few types of interventions, including an
explicit focus on model-building and/or developing expert-like beliefs
that appear to lead to significant improvements in beliefs. Further,
small courses and those for elementary education and non-science majors
also result in improved beliefs. However, because the available data
oversamples certain types of classes, it is unclear what leads to these
improvements. The figure below depicts CLASS and MPEX shifts for a
variety of teaching
methods.![MPEXTM](https://www.physport.org/physport/images/tiny/CLASS%20Plot%20GPER.png)

# Resources

## What's a good introductory article?

E. Redish, J. Saul, and R. Steinberg, [Student expectations in
introductory
physics](http://www.per-central.org/items/detail.cfm?ID=2819), Am. J.
Phys. **66** (3), 212 (1998).

# Background

## Research: What research has been done to create and validate the test?

### Research Overview

Test questions were chosen through literature review, discussion with
faculty and the researchers’ personal experiences. Over 100 hours of
student interviews were conducted to validate that students read and
interpreted the questions in the way intended. Redish et al. (1997)
collected MPEX data from calibration groups with varying expertise in
physics to confirm that MPEX scores increased with increasing experience
in physics. They also found that as level of expertise in physics
increased so did MPEX overall score (test was only given to each
calibration group once as a pre-test). The repeatability of test results
was tested by comparing the test average and distribution of shifts in
the same course at the same institution but for subsequent semesters.
The results were comparable. The internal consistency of the entire test
was found to be good and higher than the internal consistency of the
question clusters. Clusters of questions were decided a priori by
researchers, a deliberate decision consistent with the researchers’
resources theoretical viewpoint in which students’ beliefs are viewed as
local coherences, not stable mental structures (Elby, 2010). Because of
this theoretical stance, factors produced by standard factor analysis of
student responses do not necessarily need to exactly match the
categories the researchers defined. Saul (1998) conducted a factor
analysis of MPEX student responses and found four factors which did not
directly correspond to the researchers categories, but this does not
decrease the reliability of these categories.

### Research Validation

-   Questions based on research into student thinking
-   Student interviews
-   Expert review
-   Appropriate statistical analysis
-   At least one peer-reviewed publication
-   Administered at multiple institutions
-   Research published by someone other than developers

## Developer: Who developed this test?

E. F. Redish, J. M. Saul, & R. N. Steinberg

## Translations: Where can I find translations of this test in other languages?

-   **en**:

# References

-   A. Elby, [Helping physics students learn how to
    learn](http://www.per-central.org/items/detail.cfm?ID=2300), Am.
    J. Phys. **69** (S1), S54 (2001).
-   J. Marx and K. Cummings, [What Factors Really Influence Shifts in
    Students' Attitudes and Expectations in an Introductory Physics
    Course?](http://www.per-central.org/items/detail.cfm?ID=5278),
    presented at the Physics Education Research Conference 2006,
    Syracuse, New York, 2006.
-   C. Omasits and D. Wagner, [Investigating the Validity of the MPEX
    Survey](http://www.per-central.org/items/detail.cfm?ID=8980),
    presented at the Physics Education Research Conference 2005, Salt
    Lake City, Utah, 2005.
-   E. Redish, J. Saul, and R. Steinberg, [Student expectations in
    introductory
    physics](http://www.per-central.org/items/detail.cfm?ID=2819), Am.
    J. Phys. **66** (3), 212 (1998).
-   J. Saul, [Beyond problem solving: Evaluating introductory physics
    courses through the hidden
    curriculum](http://www.per-central.org/items/detail.cfm?ID=4297),
    Dissertation, University of Maryland, 1998.
-   J. Saul, R. Steinberg, and E. Redish, [Comparison of student
    expectations in introductory calculus-based physics
    courses](http://www.per-central.org/items/detail.cfm?ID=2862), AAPT
    Announcer **26** (2), 98 (1996).
-   U. Wutchana and N. Emarat, [Student effort expectations and their
    learning in first-year introductory physics: A case study in
    Thailand](http://www.per-central.org/items/detail.cfm?ID=11952), Phys.
    Rev. ST Phys. Educ. Res. **7** (1), 010111 (2011).

Guide Last Updated: Tue Jul 07 2015

*MPEX © 1997 E. F. Redish, J. M. Saul, & R. N. Steinberg; PhysPort
Assessment Guide © 2015 PhysPort.org*
